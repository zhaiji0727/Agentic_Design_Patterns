{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3NBB8hsfc7iJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model initialized: gemini-2.5-flash\n",
      "\n",
      "--- Running Reflection Example for Product: 'A mug that keeps coffee hot and can be controlled by a smartphone app.' ---\n",
      "\n",
      "--- Final Refined Product Description ---\n",
      "Here's a refined product description, incorporating the suggestions from the critique:\n",
      "\n",
      "---\n",
      "\n",
      "**Your Coffee, Always Perfect: The Smart Mug.**\n",
      "\n",
      "Say goodbye to lukewarm disappointment. This innovative Smart Mug intelligently maintains your drink at your precisely chosen temperature, ensuring a consistently warm, delicious experience from the first drop to the last. Take complete control â€“ set your ideal warmth and monitor battery with intuitive ease, all from your smartphone. Elevate your daily ritual and savor every perfect sip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "# Ensure your API key environment variable is set (e.g., OPENAI_API_KEY)\n",
    "try:\n",
    "    llm = ChatOpenAI(model=\"gemini-2.5-flash\", temperature=0.7, base_url=os.getenv(\"OPENAI_API_BASE\"), api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    print(f\"Language model initialized: {llm.model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing language model: {e}\", file=sys.stderr)\n",
    "    print(\"Please ensure your OPENAI_API_KEY is set correctly.\", file=sys.stderr)\n",
    "    sys.exit(1) # Exit if the LLM cannot be initialized\n",
    "\n",
    "\n",
    "# --- Define Chain Components ---\n",
    "\n",
    "# 1. Initial Generation: Creates the first draft of the product description.\n",
    "# The input to this chain will be a dictionary, so we update the prompt template.\n",
    "generation_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Write a short, simple product description for a new smart coffee mug.\"),\n",
    "        (\"user\", \"{product_details}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 2. Critique: Evaluates the generated description and provides feedback.\n",
    "critique_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Critique the following product description based on clarity, conciseness, and appeal.\n",
    "        Provide specific suggestions for improvement.\"\"\"),\n",
    "        # This will receive 'initial_description' from the previous step.\n",
    "        (\"user\", \"Product Description to Critique:\\n{initial_description}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 3. Refinement: Rewrites the description based on the original details and the critique.\n",
    "refinement_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Based on the original product details and the following critique,\n",
    "        rewrite the product description to be more effective.\n",
    "\n",
    "        Original Product Details: {product_details}\n",
    "        Critique: {critique}\n",
    "\n",
    "        Refined Product Description:\"\"\"),\n",
    "        (\"user\", \"Please provide the refined product description.\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# --- Build the Full Reflection Chain (Refactored) ---\n",
    "# This chain is structured to be more readable and linear.\n",
    "full_reflection_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        initial_description=generation_chain\n",
    "    )\n",
    "    | RunnablePassthrough.assign(\n",
    "        critique=critique_chain\n",
    "    )\n",
    "    | refinement_chain\n",
    ")\n",
    "\n",
    "\n",
    "# --- Run the Chain ---\n",
    "async def run_reflection_example(product_details: str):\n",
    "    \"\"\"Runs the LangChain reflection example with product details.\"\"\"\n",
    "    print(f\"\\n--- Running Reflection Example for Product: '{product_details}' ---\")\n",
    "    try:\n",
    "        # The chain now expects a dictionary as input from the start.\n",
    "        final_refined_description = await full_reflection_chain.ainvoke(\n",
    "            {\"product_details\": product_details}\n",
    "        )\n",
    "        print(\"\\n--- Final Refined Product Description ---\")\n",
    "        print(final_refined_description)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during chain execution: {e}\")\n",
    "\n",
    "test_product_details = \"A mug that keeps coffee hot and can be controlled by a smartphone app.\"\n",
    "await run_reflection_example(test_product_details)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "apple-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
